{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3qLZuJu6xjB"
      },
      "source": [
        "# Topic modeling example\n",
        "\n",
        "\n",
        "\n",
        "## Running with Colaboratory\n",
        "\n",
        "As of March 2023, this notebook runs in colaboratory.\n",
        "\n",
        "\n",
        "## Running with Anaconda / Jupyter\n",
        "\n",
        "As of March 2023, this notebook will not run in Jupyter, mainly due to colab's use of older pyLDAvis and gensim packages, which use deprecated or changed methods and attributes. A separate version of this notebook is available in the github repository which uses more recent packages, and which will run in Jupyter provided the correct versions of those packages are installed via conda.\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This exercise introduces topic modeling using the LDA (Latent Dirichlet Allocation) algorithm and the Non-negative Matrix Factorisation (NMF) algorithm.\n",
        "\n",
        "Topic modeling is an unsupervised approach that allows you to explore large text collections.\n",
        "\n",
        "In this example, we use the gensim LDA and pyLDAvis implementations for one type of analysis, as well as the sklearn implementations of LDA and NMF to look at how the models relate to categories in the data.\n",
        "\n",
        "The main packages that are used in this example are:\n",
        "\n",
        "nltk: http://www.nltk.org/ - for preprocessing\n",
        "\n",
        "gensim: https://radimrehurek.com/gensim/ - for building the LDA model\n",
        "\n",
        "pyLDAvis: https://github.com/bmabey/pyLDAvis - for visualization and easier exploration of the generated topics\n",
        "\n",
        "sklearn: https://scikit-learn.org/stable/, https://scikit-learn.org/stable/modules/decomposition.html#nmf - for building LDA and NMF models\n",
        "\n",
        "The example is inspired by, and uses functions from: http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
        "and\n",
        "https://github.com/derekgreene/topic-model-tutorial/blob/master/2%20-%20NMF%20Topic%20Models.ipynb\n",
        "\n",
        "Written by: Sumithra Velupillai, with input from Sonia Priou, February 2019, updated 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_UnZReMK6xjD",
        "outputId": "68336575-5143-489e-cb48-41a14d6ea136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==2.1.2\n",
            "  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (0.45.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (2.2.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (1.4.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (3.1.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (2.10.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (8.3.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==2.1.2) (1.0.0)\n",
            "Collecting funcy (from pyLDAvis==2.1.2)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2025.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->pyLDAvis==2.1.2) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest->pyLDAvis==2.1.2) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->pyLDAvis==2.1.2) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.17.0->pyLDAvis==2.1.2) (1.17.0)\n",
            "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97717 sha256=db28a29dc3946692f4523a3dc67e009fe93bda7daef37fc64da6cf40bd0217ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/88/c8/ab7982c442ee90375cb2233818b57154fa632803ffde99ed3b\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-2.1.2\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.19 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-27 13:32:48.399840\n"
          ]
        }
      ],
      "source": [
        "## First we need to import all the necessary packages\n",
        "\n",
        "# There is an incompatibility between the current pyLDAvis and pandas,\n",
        "# we will downgrade to solve it\n",
        "!pip install --upgrade pyLDAvis==2.1.2\n",
        "!pip install --upgrade pandas==1.5.3\n",
        "\n",
        "import string\n",
        "from gensim import models\n",
        "from gensim.corpora import Dictionary, MmCorpus\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "import itertools\n",
        "import zipfile\n",
        "\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError as e:\n",
        "    !pip install pyldavis\n",
        "    import pyLDAvis\n",
        "\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import codecs\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "from datetime import datetime\n",
        "print(datetime.now())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKhSKi26xjE"
      },
      "source": [
        "# 1: corpus\n",
        "The first step in building a topic model is to read a corpus, or a collection of documents.\n",
        "\n",
        "In this example, we are using documents from http://www.mtsamples.com/.\n",
        "\n",
        "These are transcribed medical transcriptions sample reports and examples from a variety of clinical disciplines, such as radiology, surgery, discharge summaries. Note that one document can belong to several categories.\n",
        "\n",
        "We will save each document, all its words, and which clinical specialty it belongs to, in a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2yvrriT6xjF"
      },
      "outputs": [],
      "source": [
        "xlds = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/topic_modelling/mtsamples_for_topic_modelling.xlsx?raw=true'\n",
        "df = pd.read_excel(xlds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8OKWscR6xjF"
      },
      "source": [
        "How many documents are in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d68eaJxt6xjF",
        "outputId": "7a333a52-995c-4ea3-fc67-42dd1feab127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6385"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRR0bxm96xjG"
      },
      "source": [
        "How many clinical specialties are in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bz2UqFGK6xjG",
        "outputId": "969aa48d-4945-4e61-f323-c4d2d1e378d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85-surgery                   1103\n",
              "97-consult                    516\n",
              "97-consult-historyandphy.     516\n",
              "6-cardiovascularpulmonary     372\n",
              "49-orthopedic                 355\n",
              "95-radiology                  273\n",
              "98-generalmedicine            259\n",
              "98-general                    259\n",
              "24-gastroenterology           230\n",
              "42-neurology                  223\n",
              "91-soapchartprogressnotes     166\n",
              "45-obstetricsgynecology       160\n",
              "82-urology                    158\n",
              "89-discharge                  108\n",
              "89-dischargesummary           108\n",
              "100-ent-otolaryngology         98\n",
              "100-ent                        98\n",
              "43-neurosurgery                94\n",
              "96-hematology-oncology         90\n",
              "96-hematology                  90\n",
              "46-ophthalmology               83\n",
              "41-nephrology                  81\n",
              "93-emergency                   75\n",
              "93-emergencyroomreports        75\n",
              "66-pediatrics-neonatal         70\n",
              "66-pediatrics                  70\n",
              "105-pain                       62\n",
              "105-painmanagement             62\n",
              "72-psychiatrypsychology        53\n",
              "87-officenotes                 51\n",
              "87-office                      51\n",
              "71-podiatry                    47\n",
              "18-dermatology                 29\n",
              "17-dentistry                   27\n",
              "70-cosmeticplasticsurgery      27\n",
              "86-letters                     23\n",
              "68-physical                    21\n",
              "68-physicalmedicine-rehab      21\n",
              "78-sleep                       20\n",
              "78-sleepmedicine               20\n",
              "21-endocrinology               19\n",
              "5-bariatrics                   18\n",
              "90-ime-qme-workcompetc.        16\n",
              "90-ime-qme-work                16\n",
              "99-chiropractic                14\n",
              "77-rheumatology                10\n",
              "44-dietsandnutritions          10\n",
              "106-speech-language             9\n",
              "92-labmedicine-pathology        8\n",
              "94-autopsy                      8\n",
              "3-allergyimmunology             7\n",
              "34-hospice-palliativecare       6\n",
              "Name: Category, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85-surgery</th>\n",
              "      <td>1103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97-consult</th>\n",
              "      <td>516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97-consult-historyandphy.</th>\n",
              "      <td>516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6-cardiovascularpulmonary</th>\n",
              "      <td>372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49-orthopedic</th>\n",
              "      <td>355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95-radiology</th>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98-generalmedicine</th>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98-general</th>\n",
              "      <td>259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24-gastroenterology</th>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42-neurology</th>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91-soapchartprogressnotes</th>\n",
              "      <td>166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45-obstetricsgynecology</th>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82-urology</th>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89-discharge</th>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89-dischargesummary</th>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100-ent-otolaryngology</th>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100-ent</th>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43-neurosurgery</th>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96-hematology-oncology</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96-hematology</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46-ophthalmology</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41-nephrology</th>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93-emergency</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93-emergencyroomreports</th>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66-pediatrics-neonatal</th>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66-pediatrics</th>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105-pain</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105-painmanagement</th>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72-psychiatrypsychology</th>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87-officenotes</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87-office</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71-podiatry</th>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18-dermatology</th>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17-dentistry</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70-cosmeticplasticsurgery</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86-letters</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68-physical</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68-physicalmedicine-rehab</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78-sleep</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78-sleepmedicine</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21-endocrinology</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5-bariatrics</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90-ime-qme-workcompetc.</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90-ime-qme-work</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99-chiropractic</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77-rheumatology</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44-dietsandnutritions</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106-speech-language</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92-labmedicine-pathology</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94-autopsy</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3-allergyimmunology</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34-hospice-palliativecare</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df['Category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAr61Npw6xjG"
      },
      "source": [
        "We need to convert the texts to words - let's use a very simple approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gV4cqdwK6xjG"
      },
      "outputs": [],
      "source": [
        "def getWords(row):\n",
        "\n",
        "    return [''.join(c.lower() for c in s if c not in string.punctuation) for s in nltk.word_tokenize(row)]\n",
        "\n",
        "df['Document words'] = df['Document Content'].apply(getWords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEtSk7iq6xjH"
      },
      "source": [
        "# 2 Using gensim and pyLDAVis\n",
        "\n",
        "We now need to generate representations for the vocabulary (dictionary) and the text collection (corpus)\n",
        "\n",
        "Let's use some functions that we can call later, and that we can modify later if we want.\n",
        "\n",
        "(Using all the words in the whole corpus or text collection is not typically what you want, because very common words,\n",
        "\n",
        "or very rare words will not generate good topic representations. Why?\n",
        "\n",
        "What parameters and configurations could be interesting to change below?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Xd3Id6U6xjH"
      },
      "outputs": [],
      "source": [
        "\n",
        "## functions from http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
        "\n",
        "## this function returns a set of stopwords predefined in the nltk package\n",
        "\n",
        "def nltk_stopwords():\n",
        "    return set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "## this function prepares the data and returns a dictionary and a corpus.\n",
        "## which parameters do you think would be worth modifying/experimenting with?\n",
        "\n",
        "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
        "  print('Building dictionary...')\n",
        "  dictionary = Dictionary(docs)\n",
        "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
        "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
        "  dictionary.filter_tokens(stopword_ids)\n",
        "  dictionary.compactify()\n",
        "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
        "  dictionary.compactify()\n",
        "\n",
        "  print('Building corpus...')\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "  return dictionary, corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOBxeKPp6xjH"
      },
      "outputs": [],
      "source": [
        "## now, let's use the functions we defined above to get our dictionary and corpus\n",
        "dictionary, corpus = prep_corpus(df['Document words'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZmtHEqY6xjH"
      },
      "outputs": [],
      "source": [
        "## If you want, you can save your corpus and dictionary to disk for quicker processing later\n",
        "## For colab, try this:\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive') # to mount the google drive\n",
        "# MmCorpus.serialize(\"gdrive/My Drive/Colab Notebooks/mtsamples.mm\", corpus)\n",
        "# dictionary.save(\"gdrive/My Drive/Colab Notebooks/mtsamples.dict\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf7W17Ik6xjH"
      },
      "outputs": [],
      "source": [
        "## Now we have our dictionary and corpus, let's generate an LDA model.\n",
        "## The LDA model has many parameters that can be set, all available parameters can be found here:\n",
        "## https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "\n",
        "## Here, we've set the number of topics to 10.\n",
        "\n",
        "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
        "\n",
        "## You can also save the generated model to disk if you want\n",
        "#lda.save('/Users/sumithra/DSV/MeDESTO/teaching/Farr2017/data/gensim_topic_model_data/mtsamples_20_lda.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9c9XFGa6xjI"
      },
      "outputs": [],
      "source": [
        "## you can now look at these topics by printing them from the generated model\n",
        "\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isb8dunX6xjI"
      },
      "outputs": [],
      "source": [
        "\n",
        "## It can be hard to get a good understanding of what's actually in these topics\n",
        "## Visualizations are very helpful for this, let's use a package that does this:\n",
        "\n",
        "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35fanwST6xjI"
      },
      "source": [
        "Take a look at the results. What observations do you have? What happens if you change the number of topics?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT7dN6fJ6xjI"
      },
      "source": [
        "# 3 Using sklearn and comparing with 'existing' categories\n",
        "\n",
        "Now you have seen how you can build a topic models with gensim and look at the contents visually with pyLDAVis.\n",
        "\n",
        "You can also use sklearn for topic modeling, both lda and nmf, and analyse results visually by comparing with existing categories, if you have them.\n",
        "\n",
        "NMF approaches can be very efficient, particularly with smaller datasets. Let's see what you think.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUDs8pbj6xjI"
      },
      "outputs": [],
      "source": [
        "# We need a couple of functions to visualise the data\n",
        "# Preparation for visualisation\n",
        "# Written by Sonia Priou, adaptations by Sumithra Velupillai\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "def get_topic_list(model, feature_names, no_top_words):\n",
        "    tlist = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        tlist[topic_idx]= str(\"%d: \" % (topic_idx)+\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "    return tlist\n",
        "\n",
        "\n",
        "def display_topic_representation (model,dataframe,tlist):\n",
        "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
        "    doc = np.arange(doc_topic.shape[0])\n",
        "    num_topics = doc_topic.shape[1]\n",
        "    dico = {'index': doc}\n",
        "    for n in range(num_topics):\n",
        "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
        "\n",
        "    #Max topic\n",
        "    Topic_max = []\n",
        "    for i in range(doc_topic.shape[0]):\n",
        "        if len(set(doc_topic[i])) == 1:\n",
        "            Topic_max.append(num_topics+1)\n",
        "        else:\n",
        "            Topic_max.append(doc_topic[i].argmax())\n",
        "    dico[\"Topic most represented\"] = Topic_max\n",
        "    #print(Topic_max)\n",
        "    tlist[num_topics+1] = 'NONE'\n",
        "    dico[\"Topic and its most representative words\"] = [tlist[x] for x in Topic_max]\n",
        "    df_topic = pd.DataFrame(dico)\n",
        "\n",
        "\n",
        "    #Link both DataFrame\n",
        "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
        "    df_result = df_result.sort_values('Topic most represented')\n",
        "\n",
        "    #Finding within the cluster found by LDA the original file\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(11.7, 8.27)\n",
        "    sns.set_style('whitegrid')\n",
        "    sns.countplot(y='Topic and its most representative words', data = df_result)\n",
        "    return df_result\n",
        "\n",
        "def display_file_representation (model,dataframe):\n",
        "    #Within a file, what is the slipt between topics found\n",
        "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
        "    doc = np.arange(doc_topic.shape[0])\n",
        "    no_topics = doc_topic.shape[1]\n",
        "    topic = np.arange(no_topics)\n",
        "    dico = {'index': doc}\n",
        "    for n in range(no_topics):\n",
        "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
        "    #Max topic\n",
        "    Topic_max = []\n",
        "    for i in range(doc_topic.shape[0]):\n",
        "        Topic_max.append(doc_topic[i].argmax())\n",
        "    dico[\"Topic most represented\"] = Topic_max\n",
        "    df_topic = pd.DataFrame(dico)\n",
        "    #print(df_topic)\n",
        "\n",
        "\n",
        "    #Link both DataFrame\n",
        "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
        "\n",
        "    dico2 = {'Topic': topic}\n",
        "    for i in df_result['Category'].value_counts().index:\n",
        "        ser = df_result.loc[df_result['Category']==i].mean()\n",
        "        score = ser[2:no_topics+2]\n",
        "        dico2[i]=score\n",
        "\n",
        "    df_score = pd.DataFrame(dico2)\n",
        "    print('For each given file, we calculate the mean percentage of the documents depence to each topic')\n",
        "    print('')\n",
        "    print(df_score)\n",
        "    print(df_result['Category'].value_counts().index)\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(dataframe['Category'].value_counts()))\n",
        "    count = 0\n",
        "    for i in df_result['Category'].value_counts().index:\n",
        "        sns.barplot(x='Topic', y =i ,data = df_score, ax=axs[count])\n",
        "        count = count + 1\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8Btgkm6xjJ"
      },
      "source": [
        "Let's look at a smaller sample, to make the analysis a bit easier. You can choose other categories of course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDqHuBqM6xjJ"
      },
      "outputs": [],
      "source": [
        "categories_to_keep = ['17-dentistry', '46-ophthalmology', '72-psychiatrypsychology', '71-podiatry']\n",
        "df_smaller = df.loc[df['Category'].isin(categories_to_keep)]\n",
        "#new_df = df.drop(df_smaller)\n",
        "new_df = df[~df.isin(df_smaller)]\n",
        "df_smaller['index'] = range(0,len (df_smaller))\n",
        "df_smaller.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU_8VEXO6xjJ"
      },
      "source": [
        "Now let's use sklearn's function for converting corpora to document-term-matrices. We'll define a function for this, which takes as parameters a dataframe, the name of the text column that should be transformed to tf-idf, and some optional parameters for thresholds in creating the model. These thresholds can of course be changed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6jFEIwB6xjJ"
      },
      "outputs": [],
      "source": [
        "def get_tfidf_model(dataframe, text_column, min_df = 5, max_df=100000, lowercase = True):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    min_df = 5\n",
        "    max_df = 100000\n",
        "    lowercase = True\n",
        "\n",
        "\n",
        "    bow_transformer = CountVectorizer(stop_words=stopwords,\n",
        "                                  min_df=min_df,\n",
        "                                  max_df=max_df,\n",
        "                                  lowercase = lowercase).fit(dataframe[text_column])\n",
        "    document_bow = bow_transformer.transform(dataframe[text_column])\n",
        "    feature_names = bow_transformer.get_feature_names_out()\n",
        "\n",
        "    tfidf_transformer = TfidfTransformer().fit(document_bow)\n",
        "    document_tfidf= tfidf_transformer.transform(document_bow)\n",
        "    return feature_names, document_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZNE_S_6xjK"
      },
      "source": [
        "Let's also define some functions to train the different topic models - we're using NMF and LDA with some preset parameters, these can of course be changed - check out the documentation to see what parameters are available. Note that the two functions return different variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF4FC_LZ6xjK"
      },
      "outputs": [],
      "source": [
        "def getNMFModel(no_topics, document_tfidf):\n",
        "    nmf = NMF(n_components=no_topics)\n",
        "    W = nmf.fit_transform(document_tfidf)\n",
        "    H = nmf.components_\n",
        "    return nmf, W, H\n",
        "\n",
        "def getLDAModel(no_topics, document_tfidf):\n",
        "    lda = LatentDirichletAllocation(n_components=no_topics).fit(document_tfidf)\n",
        "    return lda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBiqzcP16xjK"
      },
      "source": [
        "Now let's convert our data to a tfidf model and get the feature names from that model (i.e. the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUFoWOch6xjK"
      },
      "outputs": [],
      "source": [
        "feature_names, document_tfidf = get_tfidf_model(df_smaller, 'Document Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSUlN3pr6xjK"
      },
      "source": [
        "How many features does the model contain? What parameters can you change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1mma8Gt6xjL"
      },
      "outputs": [],
      "source": [
        "print(len(feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pWIP-G6xjL"
      },
      "source": [
        "### Optional\n",
        "What's in the variable feature_names? How can you take a look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRyCmavS6xjL"
      },
      "outputs": [],
      "source": [
        "## Do something with the variable feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xapEOVqM6xjL"
      },
      "source": [
        "### Number of topics and top words for each topic\n",
        "* How many topics do you want the model to generate?\n",
        "* How many discriminative words from each topic do you want to look at?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrym3JOx6xjL"
      },
      "outputs": [],
      "source": [
        "## In this case, we know that there are four categories in the dataset,\n",
        "## let's see if the models produce something coherent with that number\n",
        "no_topics = 4\n",
        "\n",
        "## Each topic is represented with a list of words, ranked according to how discriminative they are for that topic.\n",
        "## We can use the top ranked words to try to understand what the topic represents.\n",
        "no_top_words = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv--SfSA6xjL"
      },
      "source": [
        "Now let's build an lda model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ALQ8Hjz6xjL"
      },
      "outputs": [],
      "source": [
        "lda = getLDAModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeO6AiCr6xjL"
      },
      "source": [
        "### Most discriminative words - LDA\n",
        "Let's look at the most discriminative words for each topic generated from our LDA model. Do you see a pattern? Do they make sense? Do you think more work needs to be done, e.g. with parameters, with the underlying representation, or other things?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3NnLz6b6xjM"
      },
      "outputs": [],
      "source": [
        "display_topics(lda,feature_names, no_top_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AjRL5O_6xjM"
      },
      "source": [
        "We can now look at the distribution of the main topic (i.e. the topic with highest probability) for each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7pARKF56xjM"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document in the document collection')\n",
        "tlist = get_topic_list(lda,feature_names, 10)\n",
        "df_result = display_topic_representation(lda.transform(document_tfidf),df_smaller,tlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1F7IYkx6xjM"
      },
      "source": [
        "Does this look reasonable to you? Are all the topics represented? How do you interpret these results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJPW55b_6xjM"
      },
      "source": [
        "There's a new column saved in the dataframe that contains the topic number that had the highest probability, we can look at the distribution of those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W5_sDo16xjM"
      },
      "outputs": [],
      "source": [
        "df_result['Topic most represented'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls15ARla6xjM"
      },
      "source": [
        "We can also look at the probability scores each topic resulted in in the whole document collection and get some descriptive statistics. Look at some of the other topics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A9W1wOb6xjN"
      },
      "outputs": [],
      "source": [
        "df_result['topic1'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-LiIPBr6xjN"
      },
      "source": [
        "Now let's look at the distribution of topics in the files in relation to the 'existing' categories in the dataset. We'll use the function we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdyijjEz6xjN"
      },
      "outputs": [],
      "source": [
        "display_file_representation(lda.transform(document_tfidf),df_smaller)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5kv3bYX6xjN"
      },
      "source": [
        "### NMF\n",
        "Now let's compare with NMF.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7brnyoO6xjN"
      },
      "outputs": [],
      "source": [
        "nmf, W, H = getNMFModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvidL8J36xjN"
      },
      "outputs": [],
      "source": [
        "display_topics(nmf, feature_names, no_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiCpyzpI6xjN"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document')\n",
        "\n",
        "tlist = get_topic_list(nmf,feature_names, 10)\n",
        "df_result = display_topic_representation(W,df_smaller,tlist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1tHqkPx6xjO"
      },
      "outputs": [],
      "source": [
        "display_file_representation(W,df_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80YvVm3M6xjO"
      },
      "source": [
        "## LDA vs NMF?\n",
        "What do you think about these results and models? What main differences do you notice when comparing NMF and LDA results? Do you think one is better than the other? What parameters might be worth changing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cverR3lp6xjO"
      },
      "source": [
        "# Choosing number of topics ('k') - coherence score\n",
        "\n",
        "There are several metrics proposed for automatically calculating what the 'optimal' number of topics in a document collection is, by trying to measure how coherent generated topics are.\n",
        "\n",
        "Here, we will use the TC-W2W score proposed by O'Callaghan et al.\n",
        "\n",
        "O’Callaghan, D., Greene, D., Carthy, J., & Cunningham, P. (2015).\n",
        "An analysis of the coherence of descriptors in topic modeling.\n",
        "Expert Systems with Applications, 42(13), 5645-5657\n",
        "\n",
        "The code is from: https://github.com/derekgreene/topic-model-tutorial/blob/master/3%20-%20Parameter%20Selection%20for%20NMF.ipynb\n",
        "With slight adaptations for our example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdsnmpzQ6xjO"
      },
      "source": [
        "The main idea with this score is to calculate the pairwise embedding vector similarity for each term pair in the top ranked words in each generated topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyg67c_H6xjO"
      },
      "source": [
        "We need to have a word2vec model first. We'll start by building one on the entire document collection we have. NOTE: this means that we have an 'in-domain' model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KLvi9BJ6xjO"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "w2v_model = gensim.models.Word2Vec(df['Document words'], vector_size=300, min_count=2, batch_words=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf9TGUW6xjO"
      },
      "source": [
        "How big is the vocabulary in this embedding model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8_wnfI6xjO"
      },
      "outputs": [],
      "source": [
        "print( \"Model has %d terms\" % len(w2v_model.wv) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpGjRHef6xjP"
      },
      "source": [
        "Recap: we can get similarity scores for different word pairs in these types of models. We can try this on a pair of words from the generated topics in our previous NMF model, e.g. topic3 had the words 'teeth' and 'tooth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfTPyxs6xjP"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.similarity('teeth', 'tooth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83Zp2fr6xjP"
      },
      "source": [
        "Next, we need to build topic models with different number of topics assigned. Let's define a range of k to try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acPuyze86xjP"
      },
      "outputs": [],
      "source": [
        "kmin, kmax = 3, 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4R2PZi6xjP"
      },
      "source": [
        "Let's try our smaller dataset again, and generate a tfidf-representation to use in the topic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggF74pG6xjP"
      },
      "outputs": [],
      "source": [
        "feature_names, document_tfidf = get_tfidf_model(df_smaller, 'Document Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHaNeClg6xjP"
      },
      "source": [
        "Now, let's generate NMF topic models for each value of k in our range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4LAlWmC6xjP"
      },
      "outputs": [],
      "source": [
        "from sklearn import decomposition\n",
        "topic_models = []\n",
        "# try each value of k\n",
        "for k in range(kmin,kmax+1):\n",
        "    print(\"Applying NMF for k=%d ...\" % k )\n",
        "    # run NMF\n",
        "    model, W, H =  getNMFModel(k, document_tfidf)\n",
        "    # store for later\n",
        "    topic_models.append( (k,W,H) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKyby19N6xjP"
      },
      "source": [
        "We need a couple of functions to calculate the coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC-1NI606xjP"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "def calculate_coherence( w2v_model, term_rankings , print_pairs=False):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
        "            ## check if word in vocabulary first!! Added by Sumithra\n",
        "            if pair[0] in w2v_model.wv and pair[1] in w2v_model.wv:\n",
        "                pair_scores.append( w2v_model.wv.similarity(pair[0], pair[1]) )\n",
        "                if print_pairs:\n",
        "                    print(pair[0], pair[1], w2v_model.wv.similarity(pair[0], pair[1]))\n",
        "            else:\n",
        "                if print_pairs:\n",
        "                    print('word pair not in vocabulary', pair[0], pair[1])\n",
        "                pair_scores.append( 0.0 )\n",
        "        # get the mean for all pairs in this topic\n",
        "        topic_score = sum(pair_scores) / len(pair_scores)\n",
        "        overall_coherence += topic_score\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZoQbQr6xjQ"
      },
      "outputs": [],
      "source": [
        "def get_descriptor( all_terms, H, topic_index, top ):\n",
        "    # reverse sort the values to sort the indices\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    # now get the terms corresponding to the top-ranked indices\n",
        "    top_terms = []\n",
        "    for term_index in top_indices[0:top]:\n",
        "        top_terms.append( all_terms[term_index] )\n",
        "    return top_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM8Cgidj6xjQ"
      },
      "source": [
        "Let's calculate the coherence score for each model with k topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-tEMfAk6xjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "k_values = []\n",
        "coherences = []\n",
        "for (k,W,H) in topic_models:\n",
        "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
        "    term_rankings = []\n",
        "    for topic_index in range(k):\n",
        "        term_rankings.append( get_descriptor( feature_names, H, topic_index, 10 ) )\n",
        "    # Now calculate the coherence based on our Word2vec model\n",
        "    k_values.append( k )\n",
        "    coherences.append( calculate_coherence( w2v_model, term_rankings, print_pairs=False ) )\n",
        "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEFnTc4e6xjQ"
      },
      "source": [
        "We can plot this to visualise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPguO5M6xjQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "matplotlib.rcParams.update({\"font.size\": 14})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ1oD3eb6xjQ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(13,7))\n",
        "# create the line plot\n",
        "ax = plt.plot( k_values, coherences )\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Mean Coherence\")\n",
        "# add the points\n",
        "plt.scatter( k_values, coherences, s=120)\n",
        "# find and annotate the maximum point on the plot\n",
        "ymax = max(coherences)\n",
        "xpos = coherences.index(ymax)\n",
        "best_k = k_values[xpos]\n",
        "plt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_D_tcmD6xjQ"
      },
      "source": [
        "What seems to be the 'optimal' number of topics? Does this make sense do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt3UPgoC6xjQ"
      },
      "source": [
        "Now let's build a model with this 'k' and look at what the model produces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zim5f5XK6xjR"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Assign a value to no_topics from the results above\n",
        "no_topics = # fill in here\n",
        "\n",
        "## How many words do you want as topic descriptors?\n",
        "no_top_words = # fill in here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BIT7ZCO6xjR"
      },
      "outputs": [],
      "source": [
        "nmf, W, H = getNMFModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtHRMImU6xjR"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document')\n",
        "\n",
        "tlist = get_topic_list(nmf,feature_names, 10)\n",
        "df_result = display_topic_representation(W,df_smaller,tlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnvTbL2u6xjR"
      },
      "outputs": [],
      "source": [
        "df_result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSRhSrnS6xjR"
      },
      "outputs": [],
      "source": [
        "# We can also look at the distribution of topics per document.\n",
        "# We can extract the columns with topic probabilities\n",
        "# NOTE You will need to edit this to fit with the number of\n",
        "# topics you set above, and the resulting columm names!\n",
        "\n",
        "tmp = df_result[['index', 'topic0', 'topic1', 'topic2', 'topic3']]\n",
        "\n",
        "## Let's sample a few documents\n",
        "tmp = tmp.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxNvtUb6xjR"
      },
      "source": [
        "Let's plot these"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jOPPQ866xjR"
      },
      "outputs": [],
      "source": [
        "ax = tmp.plot.bar(figsize=(10,10), x='index')\n",
        "ax.legend(tlist.items(), bbox_to_anchor=(1, 0.5));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB3A5qxl6xjR"
      },
      "source": [
        "Let's look at one of these documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGK-jitb6xjR"
      },
      "outputs": [],
      "source": [
        "## choose a document index and assign value to i\n",
        "i = # fill in value here\n",
        "tmp[tmp['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAci9SiP6xjS"
      },
      "outputs": [],
      "source": [
        "## which category did it belong to?\n",
        "df_smaller[df_smaller['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DPiOj-h6xjS"
      },
      "outputs": [],
      "source": [
        "## what's in the document? does the generated topic distribution make sense?\n",
        "df_smaller[df_smaller['index']==i]['Document Content'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oaQs-526xjS"
      },
      "outputs": [],
      "source": [
        "## this is another example\n",
        "i = 72\n",
        "tmp[tmp['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpe0BPOD6xjS"
      },
      "outputs": [],
      "source": [
        "df_smaller[df_smaller['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ru2UO7f6xjS"
      },
      "outputs": [],
      "source": [
        "df_smaller[df_smaller['index']==i]['Document Content'].tolist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}